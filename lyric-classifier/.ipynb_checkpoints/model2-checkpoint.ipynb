{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyric Classifier\n",
    "---\n",
    "\n",
    "Music genre classifier (between bossa nova, funk, gospel and sertanejo styles) using lyrics.\n",
    "\n",
    "References:\n",
    "* http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "* http://zacstewart.com/2015/04/28/document-classification-with-scikit-learn.html\n",
    "* http://sebastianraschka.com/Articles/2014_naive_bayes_1.html\n",
    "* http://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/\n",
    "* _\n",
    "* http://radimrehurek.com/data_science_python/\n",
    "* https://spandan-madan.github.io/DeepLearningProject/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Data Wrangling\n",
    "\n",
    "### Load and join the data lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bossa_nova = pd.read_csv('input/lyrics/bossa_nova.csv')\n",
    "bossa_nova['genre'] = 'bossa_nova'\n",
    "\n",
    "funk = pd.read_csv('input/lyrics/funk.csv')\n",
    "funk['genre'] = 'funk'\n",
    "\n",
    "gospel = pd.read_csv('input/lyrics/gospel.csv')\n",
    "gospel['genre'] = 'gospel'\n",
    "\n",
    "sertanejo = pd.read_csv('input/lyrics/sertanejo.csv')\n",
    "sertanejo['genre'] = 'sertanejo'\n",
    "\n",
    "df = pd.concat([bossa_nova, funk, gospel, sertanejo], ignore_index=True)\n",
    "df = df.reindex(np.random.permutation(df.index)) # Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  lyric       genre\n",
      "2292   \\nHá quem amou demais\\nHá quem chorou demais\\...      gospel\n",
      "2214   \\nQuão grande graça após uma consagração\\nFei...      gospel\n",
      "3082   \\nEu preciso daquele sorriso\\nQue me envolve ...   sertanejo\n",
      "1874   \\nRenova-me, Senhor Jesus\\nJá não quero ser i...      gospel\n",
      "304    \\nOn my way, (on my way)\\nI laugh with you, (...  bossa_nova\n",
      "804    \\nÉ a flauta envolvente que mexe com a mente\\...        funk\n",
      "1637   \\nEm poucas palavras\\nVou dizer quem você é\\n...      gospel\n",
      "744    \\nNem que algum dia eu venha a chorar\\nE vive...  bossa_nova\n",
      "1401   \\nAcabou o \"caô\"\\nO guerrero chegou\\nO guerre...        funk\n",
      "1534   \\nMas é que eu sou safado\\nBandido, tarado\\nE...        funk\n"
     ]
    }
   ],
   "source": [
    "# Data Analysis\n",
    "#print( df.describe() )\n",
    "\n",
    "print( df.head( n=10 ) )\n",
    "\n",
    "#df['genre'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "Load stopwords (pt-BR), preparing tokenizer, vectorizers and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words ['de ', 'a ', 'o ', 'que ', 'e '] ...\n"
     ]
    }
   ],
   "source": [
    "with open('stopwords.txt', 'r') as infile:\n",
    "    stop_words = infile.read().splitlines()\n",
    "print('stop words %s ...' %stop_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don', 't', 'want', 'swim']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def porter_tokenizer(text, stemmer=porter_stemmer):\n",
    "    \"\"\"\n",
    "    A Porter-Stemmer-Tokenizer hybrid to splits sentences into words (tokens) \n",
    "    and applies the porter stemming algorithm to each of the obtained token. \n",
    "    Tokens that are only consisting of punctuation characters are removed as well.\n",
    "    Only tokens that consist of more than one letter are being kept.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        \n",
    "    text : `str`. \n",
    "      A sentence that is to split into words.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    \n",
    "    no_punct : `str`. \n",
    "      A list of tokens after stemming and removing Sentence punctuation patterns.\n",
    "    \n",
    "    \"\"\"\n",
    "    lower_txt = text.lower()\n",
    "    tokens = nltk.wordpunct_tokenize(lower_txt)\n",
    "    stems = [porter_stemmer.stem(t) for t in tokens]\n",
    "    no_punct = [s for s in stems if re.match('^[a-zA-Z]+$', s) is not None]\n",
    "    return no_punct\n",
    "\n",
    "# Test tokenizer\n",
    "porter_tokenizer(\"Don't !!! --- want swimming. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(\n",
    "            encoding='utf-8',\n",
    "            decode_error='replace',\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            binary=False,\n",
    "            stop_words=stop_words,\n",
    "            tokenizer=porter_tokenizer,\n",
    "            ngram_range=(1,1)\n",
    "    )\n",
    "\n",
    "vec2 = CountVectorizer(\n",
    "            encoding='utf-8',\n",
    "            decode_error='replace',\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            binary=False,\n",
    "            stop_words=stop_words,\n",
    "            tokenizer=porter_tokenizer,\n",
    "            ngram_range=(2,2)\n",
    "    )\n",
    "\n",
    "#X_train = df['lyric'].values\n",
    "#vec = vec.fit(X_train.ravel())\n",
    "#print('Vocabulary size: %s' %len(vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "            encoding='utf-8',\n",
    "            decode_error='replace',\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            binary=False,\n",
    "            stop_words=stop_words,\n",
    "            tokenizer=porter_tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2880,) (2880,)\n",
      "(320,) (320,)\n"
     ]
    }
   ],
   "source": [
    "# Split data (validation accuracy)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df['lyric'].values\n",
    "y = df['genre'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2880, 18404)\n"
     ]
    }
   ],
   "source": [
    "# Data Prep\n",
    "\n",
    "# Count Vec\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#count_vectorizer = CountVectorizer(ngram_range=(1, 4))\n",
    "#X_train_counts = count_vectorizer.fit_transform(df['lyric'].values)\n",
    "#counts.shape\n",
    "\n",
    "X_train_counts = vec.fit_transform(X_train.ravel())\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "#X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "#X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ' \\nE ela vem toda indisciplinada\\nEla gosta de dar uma quicada\\nEla quica, ela para, ela fica\\nEsfola a cabeça da pica\\nEla quica, ela para, ela fica\\nEsfola a cabeça da pica\\nCaralho, caralho!\\nSentou na piroca cheia de violência\\nE quase me machucou!\\nE ela só tem 16 e te acaba com uma sentada\\nUi! Caraca, moleque! Que pepeca malcriada!\\nEla só tem 16 e te acaba numa sentada\\nCaraca, moleque! Que pepeca malcriada!\\nSenta, garota. Vem sentar com força!\\nE ela vem toda indisciplinada\\nEla gosta de dar uma quicada\\nEla quica, ela para, ela fica\\nEsfola a cabeça da pica\\nE ela só tem 16 e te acaba com uma sentada...\\nUi! Caraca, moleque! Que pepeca malcriada!\\nEla só tem 16 e te acaba numa sentada\\nCaraca, moleque! Que pepeca malcriada!\\nCaralho, caralho! Sentou na piroca cheia de violência\\nE quase me machucou! '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b556cd781a29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Avaliacao do modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/Toshiba/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \"\"\"\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/Toshiba/anaconda/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"classes_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[1;32m    708\u001b[0m                 self.class_log_prior_)\n",
      "\u001b[0;32m/Volumes/Toshiba/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    380\u001b[0m                                       force_all_finite)\n\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: ' \\nE ela vem toda indisciplinada\\nEla gosta de dar uma quicada\\nEla quica, ela para, ela fica\\nEsfola a cabeça da pica\\nEla quica, ela para, ela fica\\nEsfola a cabeça da pica\\nCaralho, caralho!\\nSentou na piroca cheia de violência\\nE quase me machucou!\\nE ela só tem 16 e te acaba com uma sentada\\nUi! Caraca, moleque! Que pepeca malcriada!\\nEla só tem 16 e te acaba numa sentada\\nCaraca, moleque! Que pepeca malcriada!\\nSenta, garota. Vem sentar com força!\\nE ela vem toda indisciplinada\\nEla gosta de dar uma quicada\\nEla quica, ela para, ela fica\\nEsfola a cabeça da pica\\nE ela só tem 16 e te acaba com uma sentada...\\nUi! Caraca, moleque! Que pepeca malcriada!\\nEla só tem 16 e te acaba numa sentada\\nCaraca, moleque! Que pepeca malcriada!\\nCaralho, caralho! Sentou na piroca cheia de violência\\nE quase me machucou! '"
     ]
    }
   ],
   "source": [
    "# Classifier #1: NaiveBayes (Multinomial)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf1 = MultinomialNB().fit(X_train_counts, y_train)\n",
    "\n",
    "# Avaliacao do modelo\n",
    "y_pred = clf1.predict(X_test)\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classifier #1: NaiveBayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#clf = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "# Avaliacao do modelo\n",
    "#y_pred = clf.predict(X_test)\n",
    "#metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8359375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2', \n",
    "                     alpha=1e-4, n_iter=500, random_state=42)\n",
    "clf2 = clf.fit(X_train, y_train)\n",
    "\n",
    "# Avaliacao do modelo\n",
    "y_pred = clf2.predict(X_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "#print(metrics.classification_report(y_test, y_pred, target_names=df['genre'].values))\n",
    "\n",
    "#print( metrics.confusion_matrix(y_test, y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#for i in [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]:\n",
    "clf = LogisticRegression(C = 10).fit(X_train, y_train)\n",
    "\n",
    "# Avaliacao do modelo\n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "pipeline_3 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "parameters_3 = dict(\n",
    "    vect__binary=[False],\n",
    "    vect__stop_words=[stop_words, None],\n",
    "    vect__tokenizer=[porter_tokenizer, None],\n",
    "    vect__ngram_range=[(1,1), (2,2), (3,3)],\n",
    ")\n",
    "\n",
    "grid_search_3 = GridSearchCV(pipeline_3, parameters_3, n_jobs=1)\n",
    "\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline_3.steps])\n",
    "print(\"parameters:\")\n",
    "grid_search_3.fit(X_train, y_train)\n",
    "print(\"Best score: %0.3f\" % grid_search_3.best_score_)\n",
    "#print(\"Best parameters set:\")\n",
    "#best_parameters_3 = grid_search_3.best_estimator_.get_params()\n",
    "#for param_name in sorted(parameters_3.keys()):\n",
    "#    print(\"\\t%s: %r\" % (param_name, best_parameters_3[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
